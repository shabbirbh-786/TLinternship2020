{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapper import get_links_and_save_content\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "from queue import Queue\n",
    "import pickle\n",
    "from collections import defaultdict \n",
    "\n",
    "web_content_directory = 'WebContent3000'\n",
    "seed_url = '<give the seed URL>'\n",
    "pages_to_crawl = 3000\n",
    "domain = '<give the domain name>'\n",
    "web_graph = 'WebGraph3000'\n",
    "visited_url = 'VisitedUrl3000'\n",
    "visiting_url = 'VisitingUrl3000'\n",
    "\n",
    "class WebCrawler:\n",
    "    \n",
    "    def __init__(self,base_url,domain,pages_to_crawl=3000):\n",
    "        \n",
    "        self.visiting_url=[]\n",
    "        self.visited_url=set([])\n",
    "        self.pages_to_crawl = pages_to_crawl\n",
    "        self.visiting_url.append(base_url)\n",
    "        self.domain=domain\n",
    "        self.web_content_directory = web_content_directory\n",
    "        self.page_links_dict = defaultdict(list)\n",
    "        \n",
    "    def parse_url(self,base_url,links):\n",
    "        for link in links:\n",
    "            url = link['href']\n",
    "            if url != None and (url.startswith('/') or self.domain in url) and self. is_valid_extension(url) and '@' not in url:\n",
    "                url = urljoin(base_url,url)\n",
    "                if url[-1]=='/':\n",
    "                    url= url[:-1]\n",
    "                if 'https' not in url:\n",
    "                    url = url.replace(\"http\", \"https\")\n",
    "                self.page_links_dict[base_url].append(url)\n",
    "                if url not in self.visited_url and url not in self.visiting_url:\n",
    "                    self.visiting_url.append(url)\n",
    "                    print(url)        \n",
    "        \n",
    "    def start_crawling(self):\n",
    "        \n",
    "        crawled_page_count = 0\n",
    "        \n",
    "        while crawled_page_count < self.pages_to_crawl and len(self.visiting_url)>0:\n",
    "            url = self.visiting_url[0]\n",
    "            self.visiting_url=self.visiting_url[1:]\n",
    "            self.visited_url.add(url)\n",
    "            filename = self.web_content_directory + '/' + str(crawled_page_count)+'.txt'\n",
    "            try:\n",
    "                success,links = get_links_and_save_content(url,filename)\n",
    "                \n",
    "                print('-----------------links form {} this url----------------'.format(url))\n",
    "                if success:\n",
    "                    self.parse_url(url,links)\n",
    "                    crawled_page_count += 1\n",
    "                print('-----------------------------links ended------------------------------')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                with open(web_graph + '.json', 'w+') as fd:\n",
    "                    json.dump(self.page_links_dict, fd)\n",
    "\n",
    "                with open(visited_url, 'wb') as fv:\n",
    "                    pickle.dump(self.visited_url, fv)\n",
    "                    \n",
    "                with open(visiting_url, 'wb') as fvl:\n",
    "                    pickle.dump(self.visiting_url,fvl)\n",
    "                \n",
    "                \n",
    "        with open(web_graph +'.json', 'w+') as fd:\n",
    "                json.dump(self.page_links_dict, fd)\n",
    "\n",
    "        with open(visited_url, 'wb') as fv:\n",
    "                pickle.dump(self.visited_url, fv)\n",
    "                    \n",
    "        with open(visiting_url, 'wb') as fvl:\n",
    "                pickle.dump(self.visiting_url,fvl)\n",
    "                \n",
    "    def is_valid_extension(self,url):\n",
    "        invalid_extensions = [\"pdf\", \"jpg\", \"jpeg\", \"doc\", \"docx\", \"ppt\", \"pptx\", \"png\", \"txt\", \"exe\", \"ps\", \"psb\"]\n",
    "        if True in [ext in url for ext in invalid_extensions]:\n",
    "            return False\n",
    "        return True\n",
    "            \n",
    "            \n",
    "WebCrawler(seed_url,domain,pages_to_crawl,3000).start_crawling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
