{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Chemical compunds are used for preparing bombs based on some reactions : 2 \n",
      " Cricket is a boring game where the batsman only enjoys the game : 1 \n",
      " Machine learning is a area of Artificial intelligence : 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    " \n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    " \n",
    "# Cleaning the text sentences so that punctuation marks, stop words &amp; digits are removed\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    processed = re.sub(r\"\\d+\",\"\",normalized)\n",
    "    y = processed.split()\n",
    "    return y\n",
    "\n",
    "path = \"<path to the files where content of website are store along with URL>\"\n",
    " \n",
    "train_clean_sentences = []\n",
    "fp = open(path,'r')\n",
    "for line in fp:\n",
    "    line = line.strip()\n",
    "    cleaned = clean(line)\n",
    "    cleaned = ' '.join(cleaned)\n",
    "    train_clean_sentences.append(cleaned)\n",
    " \n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(train_clean_sentences)\n",
    "\n",
    "# Clustering the website contents sentences with K-means technique\n",
    "modelkmeans = KMeans(n_clusters=2, init='k-means++', max_iter=200, n_init=100)\n",
    "modelkmeans.fit(X)\n",
    " \n",
    "def clean_senetnce_transform(sentences):\n",
    "    test_clean_sentence = []\n",
    "    for test in sentences:\n",
    "        cleaned_test = clean(test)\n",
    "        cleaned = ' '.join(cleaned_test)\n",
    "        cleaned = re.sub(r\"\\d+\",\"\",cleaned)\n",
    "        test_clean_sentence.append(cleaned)\n",
    "    return vectorizer.transform(test_clean_sentence)\n",
    "\n",
    "\n",
    "def get_output(test_sentences,modelkmeans):\n",
    "    Test = clean_senetnce_transform(test_sentences)\n",
    "    predicted_labels_kmeans = modelkmeans.predict(Test)\n",
    "    print(predicted_labels_kmeans)\n",
    "    \n",
    "    \n",
    "# in this test_sentences we will pass the whole content of all the URL which are related to COVID-19 and chloroquine\n",
    "# this will give the output depending on which cluster it belongs to.\n",
    "get_output(test_sentences,modelkmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
